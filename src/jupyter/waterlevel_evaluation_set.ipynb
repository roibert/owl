{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "matplotlib.rcParams.update(\n",
    "    {\n",
    "        'text.usetex': False,\n",
    "        'font.family': 'stixgeneral',\n",
    "        'mathtext.fontset': 'stix',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_errors(model_evaluation):\n",
    "    model_evaluation[absolute_prediction_error] = model_evaluation[\"Percentage Full [%%]\"] - model_evaluation[\"Predictions\"]\n",
    "    model_evaluation[relative_prediction_error] = model_evaluation[absolute_prediction_error] / model_evaluation[\"Percentage Full [%%]\"] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaps = []\n",
    "\n",
    "model_folder = None\n",
    "plot_scenario += 1\n",
    "\n",
    "if plot_scenario == 1:\n",
    "    model_folder = \"DEFINE\"\n",
    "    gaps= []\n",
    "    scenario = \"Scenario 1 %s\"\n",
    "    \n",
    "if plot_scenario == 2:\n",
    "    model_folder = \"DEFINE\"\n",
    "    gaps = [\n",
    "        (0.575, 0.625), # DEFINE, or REMOVE\n",
    "        (0.675, 0.8) # DEFINE, or REMOVE\n",
    "        ]\n",
    "    scenario = \"Scenario 2 %s\"\n",
    "    \n",
    "if plot_scenario == 3:\n",
    "    model_folder = \"DEFINE\"\n",
    "    gaps = [\n",
    "        (0,.62), # DEFINE, or REMOVE\n",
    "        (0.81,1) # DEFINE, or REMOVE\n",
    "        ]\n",
    "    scenario = \"Scenario 3 %s\"\n",
    "\n",
    "if plot_scenario == 4:\n",
    "    model_folder = \"DEFINE\"\n",
    "    gaps = [\n",
    "        (.62,.81) # DEFINE, or REMOVE\n",
    "        ]\n",
    "    scenario = \"Scenario 4 %s\"\n",
    "    \n",
    "positions = [\"pos3\"]\n",
    "\n",
    "model_performance = list()\n",
    "best_model_mean = None\n",
    "best_model_range = None\n",
    "best_absolute_mean = None\n",
    "\n",
    "best_absolute_quantile_range = None\n",
    "\n",
    "evaluation_key = \"_evaluation_set_result.csv\"\n",
    "\n",
    "for current_experiment in positions:\n",
    "    use_best_mean = True\n",
    "\n",
    "    absolute_prediction_error = \"Absolute Prediction Error [mm]\"\n",
    "    relative_prediction_error = \"Relative Prediction Error [%]\"\n",
    "\n",
    "    all_models = list()\n",
    "\n",
    "    for file in os.listdir(model_folder):\n",
    "\n",
    "        if not current_experiment in file:\n",
    "            continue\n",
    "\n",
    "        if evaluation_key in file:\n",
    "\n",
    "            model_name_end_index = file.index(evaluation_key)\n",
    "\n",
    "            model_name = file[0:model_name_end_index]\n",
    "            all_models.append(model_name)\n",
    "\n",
    "\n",
    "    for model in all_models:\n",
    "        \n",
    "        if not \"m_4_conv_5\" in model:\n",
    "            continue\n",
    "\n",
    "        evaluation_path = model_folder + model + evaluation_key\n",
    "        evaluation = pd.read_csv(evaluation_path)\n",
    "\n",
    "        _compute_errors(evaluation)\n",
    "\n",
    "        mean = evaluation[relative_prediction_error].pow(2).apply(np.sqrt).mean()\n",
    "        model_performance.append((mean, model))\n",
    "        absolute_mean = abs(mean)\n",
    "\n",
    "        quantiles = evaluation[relative_prediction_error].quantile([.05,.5,.95])\n",
    "\n",
    "        quantile_range = abs(quantiles[0.95] - quantiles[0.05])\n",
    "\n",
    "        print(\"%s mean error: %.5f\" % (model, mean))\n",
    "        print(\"%s quantile range: %.5f\\n\" % (model, quantile_range))\n",
    "\n",
    "        if best_absolute_mean is None or absolute_mean < best_absolute_mean:\n",
    "            best_absolute_mean = absolute_mean\n",
    "            best_model_mean = model\n",
    "\n",
    "        if best_absolute_quantile_range is None or quantile_range < best_absolute_quantile_range:\n",
    "            best_absolute_quantile_range = quantile_range\n",
    "            best_model_range = model\n",
    "\n",
    "    print(\"Best model (mean) is: %s with mean error: %f\" % (best_model_mean, best_absolute_mean))\n",
    "    print(\"Best model (quantile range) is: %s with mean error: %f\" % (best_model_range, best_absolute_quantile_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = np.array(model_performance)\n",
    "performance = pd.DataFrame({\"Name\" : performance[:,1], \"Error\": performance[:,0]})\n",
    "performance = performance.astype({\"Error\" : \"float\"})\n",
    "performance.sort_values(by=\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = None\n",
    "\n",
    "if use_best_mean:\n",
    "    model_name = best_model_mean\n",
    "else:\n",
    "    model_name = best_model_range\n",
    "\n",
    "evaluation_path = model_folder + model_name + evaluation_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.read_csv(evaluation_path)\n",
    "_compute_errors(evaluation)\n",
    "\n",
    "print(\"Model: '%s'\" % model_name)\n",
    "\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = evaluation.sort_values(by=\"Percentage Full [%%]\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_quantile = .05\n",
    "upper_quantile = .95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_error_quantiles = evaluation[absolute_prediction_error].quantile([lower_quantile, upper_quantile])\n",
    "absolute_error_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_error_quantiles = evaluation[relative_prediction_error].quantile([lower_quantile, upper_quantile])\n",
    "relative_error_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrmse = evaluation[relative_prediction_error].pow(2).apply(np.sqrt).mean()\n",
    "rrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Gap Analysis\")\n",
    "\n",
    "rolling_window_size = 50\n",
    "\n",
    "figure, axes = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(15,10))\n",
    "figure.tight_layout()\n",
    "\n",
    "evaluation[\"Percentage Full [%%]\"].plot(ax=axes[0], label=\"Measurements\", title=\"Measurements vs. Predictions\")\n",
    "evaluation[\"Predictions\"].plot(ax=axes[0])\n",
    "\n",
    "axes[0].legend()\n",
    "\n",
    "evaluation[absolute_prediction_error].plot(ax=axes[1], title=absolute_prediction_error)\n",
    "evaluation[absolute_prediction_error].rolling(rolling_window_size).mean().plot(ax=axes[1], title=absolute_prediction_error)\n",
    "\n",
    "axes[1].axhline(evaluation[absolute_prediction_error].mean(), color=\"green\")\n",
    "axes[1].axhline(absolute_error_quantiles[lower_quantile], color=\"red\")\n",
    "axes[1].axhline(absolute_error_quantiles[upper_quantile], color=\"red\")\n",
    "\n",
    "axes[1].axhline(min(evaluation[absolute_prediction_error]), color=\"black\")\n",
    "axes[1].axhline(max(evaluation[absolute_prediction_error]), color=\"black\")\n",
    "\n",
    "evaluation[relative_prediction_error].plot(ax=axes[2], title=relative_prediction_error)\n",
    "evaluation[relative_prediction_error].rolling(rolling_window_size).mean().plot(ax=axes[2], title=relative_prediction_error)\n",
    "\n",
    "axes[2].axhline(evaluation[relative_prediction_error].mean(), color=\"green\")\n",
    "axes[2].axhline(relative_error_quantiles[lower_quantile], color=\"red\")\n",
    "axes[2].axhline(relative_error_quantiles[upper_quantile], color=\"red\")\n",
    "\n",
    "axes[2].axhline(min(evaluation[relative_prediction_error]), color=\"black\")\n",
    "axes[2].axhline(max(evaluation[relative_prediction_error]), color=\"black\")\n",
    "\n",
    "lower_gap = .7\n",
    "upper_gap = .8\n",
    "\n",
    "gap_start_x = np.argmax(evaluation[\"Percentage Full [%%]\"] >= lower_gap)\n",
    "gap_end_x = np.argmax(evaluation[\"Percentage Full [%%]\"] >= upper_gap)\n",
    "\n",
    "markers = list()\n",
    "\n",
    "for i in range(11):\n",
    "    \n",
    "    current_percentage = lower_gap + ((i) / 100)\n",
    "    marker = np.argmax(evaluation[\"Percentage Full [%%]\"] >= current_percentage)\n",
    "    markers.append(marker)\n",
    "    \n",
    "    axes[2].axvline(marker, color=\"black\", linestyle=\"--\")\n",
    "    \n",
    "axes[0].axhline(lower_gap, color=\"black\", linestyle=\"--\")\n",
    "axes[0].axhline(upper_gap, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "gap_size = (upper_gap - lower_gap) * 100\n",
    "axes[2].axhline(gap_size, color=\"black\", linestyle=\"--\")\n",
    "axes[2].axhline(-gap_size, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "\n",
    "n = len(evaluation)\n",
    "gap_start_scaled = gap_start_x / n\n",
    "gap_end_scaled = gap_end_x / n\n",
    "\n",
    "axes[2].axhline(gap_size/2, color=\"green\", linestyle=\"--\")\n",
    "axes[2].axhline(-gap_size/2, color=\"green\", linestyle=\"--\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axvline(gap_start_x, color=\"black\", linestyle=\"--\")\n",
    "    ax.axvline(gap_end_x, color=\"black\", linestyle=\"--\")\n",
    "        \n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15/1.6))\n",
    "\n",
    "start_x = int(0.9 * gap_start_x)\n",
    "end_x = int(1.1 * gap_end_x)\n",
    "\n",
    "\n",
    "plt.plot(evaluation[relative_prediction_error].iloc[start_x : end_x])\n",
    "plt.plot(evaluation[relative_prediction_error].iloc[start_x : end_x].rolling(rolling_window_size).mean())\n",
    "\n",
    "plt.hlines(0, xmin=gap_start_x, xmax=gap_end_x, color=\"red\")\n",
    "plt.hlines(y=[-5,5], xmin=gap_start_x, xmax=gap_end_x, color=\"green\", linestyle=\"--\")\n",
    "plt.hlines(y=[-10,10], xmin=gap_start_x, xmax=gap_end_x, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "\n",
    "percentage = 70\n",
    "for marker in markers:\n",
    "    plt.vlines(marker,ymin=-15, ymax=15, color=\"black\", linestyle=\"--\")\n",
    "    plt.text(marker-15, -16.0, str(percentage))\n",
    "    percentage += 1\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Intermittent Analysis\")\n",
    "\n",
    "rolling_window_size = 50\n",
    "\n",
    "figure, axes = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(15,10))\n",
    "figure.tight_layout()\n",
    "\n",
    "evaluation[\"Percentage Full [%%]\"].plot(ax=axes[0], label=\"Measurements\", title=\"Measurements vs. Predictions\")\n",
    "evaluation[\"Predictions\"].plot(ax=axes[0])\n",
    "evaluation[\"Predictions\"].rolling(rolling_window_size).mean().plot(ax=axes[0])\n",
    "\n",
    "axes[0].legend()\n",
    "\n",
    "evaluation[absolute_prediction_error].plot(ax=axes[1], title=absolute_prediction_error)\n",
    "evaluation[absolute_prediction_error].rolling(rolling_window_size).mean().plot(ax=axes[1], title=absolute_prediction_error)\n",
    "\n",
    "axes[1].axhline(evaluation[absolute_prediction_error].mean(), color=\"green\")\n",
    "axes[1].axhline(absolute_error_quantiles[lower_quantile], color=\"red\")\n",
    "axes[1].axhline(absolute_error_quantiles[upper_quantile], color=\"red\")\n",
    "\n",
    "axes[1].axhline(min(evaluation[absolute_prediction_error]), color=\"black\")\n",
    "axes[1].axhline(max(evaluation[absolute_prediction_error]), color=\"black\")\n",
    "\n",
    "evaluation[relative_prediction_error].plot(ax=axes[2], title=relative_prediction_error)\n",
    "evaluation[relative_prediction_error].rolling(rolling_window_size).mean().plot(ax=axes[2], title=relative_prediction_error)\n",
    "\n",
    "axes[2].axhline(evaluation[relative_prediction_error].mean(), color=\"green\")\n",
    "axes[2].axhline(relative_error_quantiles[lower_quantile], color=\"red\")\n",
    "axes[2].axhline(relative_error_quantiles[upper_quantile], color=\"red\")\n",
    "\n",
    "axes[2].axhline(min(evaluation[relative_prediction_error]), color=\"black\")\n",
    "axes[2].axhline(max(evaluation[relative_prediction_error]), color=\"black\")\n",
    "\n",
    "markers = list()\n",
    "    \n",
    "for axis in axes:\n",
    "    for gap in gaps:\n",
    "        gap_lower = np.argmax(evaluation[\"Percentage Full [%%]\"] >= gap[0])\n",
    "        gap_upper = np.argmax(evaluation[\"Percentage Full [%%]\"] >= gap[1])\n",
    "        axis.axvspan(gap_lower, gap_upper, alpha=.1, color=\"#d7191c\")\n",
    "\n",
    "for i in [85]:\n",
    "    current_percentage = i / 100.\n",
    "    marker = np.argmax(evaluation[\"Percentage Full [%%]\"] >= current_percentage)\n",
    "    markers.append(marker)\n",
    "    \n",
    "    for axis in axes:\n",
    "        axis.axvline(marker, color=\"black\", linestyle=\"--\")\n",
    "        if 60 <= i < 100:\n",
    "            plt.text(marker-50, -20, str(i))\n",
    "\n",
    "        \n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window_size = 50\n",
    "\n",
    "figure, axes = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(15,10))\n",
    "figure.tight_layout()\n",
    "\n",
    "evaluation[\"Percentage Full [%%]\"].plot(ax=axes[0], label=\"Measurements\", title=\"Measurements vs. Predictions\")\n",
    "evaluation[\"Predictions\"].plot(ax=axes[0], alpha=.6)\n",
    "evaluation[\"Predictions\"].rolling(rolling_window_size).mean().plot(ax=axes[0])\n",
    "\n",
    "axes[0].legend([\"Measurements\", \"Predictions\", \"Predictions (Rolling Average)\"])\n",
    "axes[0].set_ylabel(\"Percentage Full [%]\", fontsize=\"x-large\")\n",
    "\n",
    "evaluation[relative_prediction_error].plot(ax=axes[1], title=relative_prediction_error)\n",
    "evaluation[relative_prediction_error].rolling(rolling_window_size).mean().plot(ax=axes[1], title=relative_prediction_error)\n",
    "\n",
    "axes[1].axhline(evaluation[relative_prediction_error].mean(), color=\"green\")\n",
    "axes[1].axhline(relative_error_quantiles[lower_quantile], color=\"red\")\n",
    "axes[1].axhline(relative_error_quantiles[upper_quantile], color=\"red\")\n",
    "\n",
    "axes[1].axhline(min(evaluation[relative_prediction_error]), color=\"black\")\n",
    "axes[1].axhline(max(evaluation[relative_prediction_error]), color=\"black\")\n",
    "\n",
    "markers = list()\n",
    "\n",
    "axes[1].set_ylabel(\"Relative Error [%%]\", fontsize=\"x-large\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "for axis in axes:\n",
    "    for gap in gaps:\n",
    "        gap_lower = np.argmax(evaluation[\"Percentage Full [%%]\"] >= gap[0])\n",
    "        gap_upper = np.argmax(evaluation[\"Percentage Full [%%]\"] >= gap[1])\n",
    "        axis.axvspan(gap_lower, gap_upper, alpha=.1, color=\"#d7191c\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window_size = 50\n",
    "\n",
    "figure, axes = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(15,10))\n",
    "figure.tight_layout()\n",
    "\n",
    "evaluation[\"Predictions\"].plot(ax=axes, alpha=.6, color=\"#abd9e9\")\n",
    "evaluation[\"Predictions\"].rolling(rolling_window_size).mean().plot(ax=axes, color=\"#2c7bb6\")\n",
    "evaluation[\"Percentage Full [%%]\"].plot(ax=axes, label=\"Measurements\", color=\"#fdae61\")\n",
    "\n",
    "axes.legend([\"Predictions\", \"Predictions (Rolling Average)\", \"Measurements\"])\n",
    "axes.set_ylabel(\"Percentage Full [%]\", fontsize=\"x-large\")\n",
    "axes.set_xlabel(\"# Measurements\", fontsize=\"x-large\")\n",
    "\n",
    "\n",
    "\n",
    "for gap in gaps:\n",
    "    gap_lower = np.argmax(evaluation[\"Percentage Full [%%]\"] >= gap[0])\n",
    "    gap_upper = np.argmax(evaluation[\"Percentage Full [%%]\"] >= gap[1])\n",
    "    axes.axvspan(gap_lower, gap_upper, alpha=.1, color=\"#d7191c\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window_size = 100\n",
    "\n",
    "figure, axes = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(15,10))\n",
    "figure.tight_layout()\n",
    "\n",
    "evaluation[\"Predictions\"].rolling(rolling_window_size).mean().plot(ax=axes, alpha=.6, color=\"#abd9e9\")\n",
    "evaluation[\"Predictions\"].rolling(rolling_window_size).mean().plot(ax=axes, color=\"#2c7bb6\")\n",
    "evaluation[\"Percentage Full [%%]\"].rolling(rolling_window_size).mean().plot(ax=axes, label=\"Measurements\", color=\"#fdae61\")\n",
    "\n",
    "axes.legend([\"Predictions (Rolling Average)\", \"Measurements\"])\n",
    "axes.set_ylabel(\"Percentage Full [%]\", fontsize=\"x-large\")\n",
    "axes.set_xlabel(\"# Measurements\", fontsize=\"x-large\")\n",
    "\n",
    "\n",
    "\n",
    "for gap in gaps:\n",
    "    gap_lower = np.argmax(evaluation[\"Percentage Full [%%]\"] >= gap[0])\n",
    "    gap_upper = np.argmax(evaluation[\"Percentage Full [%%]\"] >= gap[1])\n",
    "    axes.axvspan(gap_lower, gap_upper, alpha=.1, color=\"#d7191c\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean relative prediction error: %.2f\" % evaluation[relative_prediction_error].mean())\n",
    "print(\"Mean relative prediction error 5th percentile: %.2f%%\" % relative_error_quantiles[lower_quantile])\n",
    "print(\"Mean relative prediction error 95th percentile: %.2f%%\" % relative_error_quantiles[upper_quantile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_bins = 21 # 5% steps\n",
    "bin_percentages = np.linspace(0,100,21)\n",
    "bin_labels = [\"L%d\" % i for i in bin_percentages]\n",
    "percentage_label = \"Percentage Label\"\n",
    "\n",
    "labels, bins = pd.cut(evaluation[\"Percentage Full [%%]\"], number_of_bins, labels=bin_labels, retbins=True)\n",
    "evaluation[percentage_label] = labels\n",
    "\n",
    "group_means = evaluation.groupby(by=percentage_label).mean()\n",
    "group_quantiles = evaluation[[percentage_label, relative_prediction_error]].groupby(by=percentage_label).quantile([.05, .95])\n",
    "\n",
    "group_quantiles.index.set_names([\"Label\", \"Quantile\"], inplace=True)\n",
    "\n",
    "upper_quantiles = group_quantiles.iloc[group_quantiles.index.get_level_values('Quantile') == upper_quantile]\n",
    "lower_quantiles = group_quantiles.iloc[group_quantiles.index.get_level_values('Quantile') == lower_quantile]\n",
    "\n",
    "lower = lower_quantiles[\"Relative Prediction Error [%]\"].values\n",
    "upper = upper_quantiles[\"Relative Prediction Error [%]\"].values\n",
    "means = group_means[\"Relative Prediction Error [%]\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10/1.6))\n",
    "fig.tight_layout()\n",
    "\n",
    "zero_line_y = np.zeros(len(bin_labels))\n",
    "\n",
    "plt.plot(bin_labels, lower)\n",
    "plt.plot(bin_labels, upper)\n",
    "plt.plot(bin_labels, means)\n",
    "\n",
    "\n",
    "plt.plot(bin_labels, zero_line_y, alpha=0)\n",
    "plt.fill_between(bin_labels, lower, upper, facecolor=\"C0\", alpha=0.05)\n",
    "plt.fill_between(bin_labels, zero_line_y, means, where=(zero_line_y < means), facecolor=\"C1\", alpha=0.35)\n",
    "plt.fill_between(bin_labels, zero_line_y, means, where=(zero_line_y > means), facecolor=\"C2\", alpha=0.35)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 10\n",
    "\n",
    "new_bin_indexes = np.linspace(0, 100, number_of_bins * factor)\n",
    "\n",
    "lower_inderpolated = np.interp(new_bin_indexes, bin_percentages, lower)\n",
    "upper_inderpolated = np.interp(new_bin_indexes, bin_percentages, upper)\n",
    "means_inderpolated = np.interp(new_bin_indexes, bin_percentages, means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10/1.6))\n",
    "fig.tight_layout()\n",
    "\n",
    "zero_line_y = np.zeros(len(lower_inderpolated))\n",
    "\n",
    "plt.plot(new_bin_indexes, lower_inderpolated)\n",
    "plt.plot(new_bin_indexes, upper_inderpolated)\n",
    "plt.plot(new_bin_indexes, means_inderpolated)\n",
    "\n",
    "plt.ylabel(\"Relative Error (Expected-Prediction) [%%]\")\n",
    "plt.xlabel(\"Percentage Full [%]\")\n",
    "\n",
    "plt.plot(new_bin_indexes, zero_line_y, alpha=0)\n",
    "plt.fill_between(new_bin_indexes, lower_inderpolated, upper_inderpolated, facecolor=\"C0\", alpha=0.05)\n",
    "plt.fill_between(new_bin_indexes, zero_line_y, means_inderpolated, where=(zero_line_y < means_inderpolated), facecolor=\"C1\", alpha=0.35)\n",
    "plt.fill_between(new_bin_indexes, zero_line_y, means_inderpolated, where=(zero_line_y > means_inderpolated), facecolor=\"C2\", alpha=0.35)\n",
    "\n",
    "plt.xticks((bin_percentages))\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 558)\n",
    "index = (evaluation[\"Percentage Full [%%]\"] > .7) & (evaluation[\"Percentage Full [%%]\"] < .8)\n",
    "evaluation[index][[\"Percentage Full [%%]\", \"Predictions\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation[index][[\"Percentage Full [%%]\", \"Predictions\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_entry = {\n",
    "    \"Title\" : scenario,\n",
    "    \"Name\" : model_name,\n",
    "    \"RRMSE\" : rrmse,\n",
    "    \"Percentiles\" : relative_error_quantiles,\n",
    "    \"Measurements\" : evaluation[\"Percentage Full [%%]\"],\n",
    "    \"Predictions\" : evaluation[\"Predictions\"],\n",
    "    \"Predictions_rolling\" : evaluation[\"Predictions\"].rolling(rolling_window_size).mean(),\n",
    "    \"Gaps\" : gaps\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_contents.append(table_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in table_contents:\n",
    "    print(t[\"Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window_size = 50\n",
    "\n",
    "n_rows = 2\n",
    "n_cols = int(len(table_contents) / n_rows)\n",
    "\n",
    "figure, axes = plt.subplots(nrows=n_rows, ncols=n_cols, sharex=False, sharey=True, figsize=(15,15),constrained_layout=True)\n",
    "\n",
    "table_content_index = 0\n",
    "\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_cols):\n",
    "        \n",
    "        table_content = table_contents[table_content_index]\n",
    "        error_string = \"\\n[%.2f, %.2f, %.2f]\" % (table_content[\"Percentiles\"][0.05], table_content[\"RRMSE\"], table_content[\"Percentiles\"][0.95])\n",
    "\n",
    "        table_content[\"Predictions\"].plot(ax=axes[i][j], color=\"#abd9e9\", alpha=.6)\n",
    "        table_content[\"Predictions_rolling\"].plot(ax=axes[i][j], color=\"#2c7bb6\")\n",
    "        table_content[\"Measurements\"].plot(ax=axes[i][j], label=\"Measurements\", color=\"#fdae61\")\n",
    "\n",
    "        axes[i][j].legend([\"Predictions\", \"Predictions (Rolling Average)\", \"Measurements\"],loc='upper left', prop={'size': 18})\n",
    "        axes[i][j].set_ylabel(\"Percentage Full [%]\", fontsize=\"xx-large\")\n",
    "\n",
    "        axes[i][j].set_xlabel(\"Measurements\", fontsize=\"x-large\")\n",
    "        axes[i][j].set_title(table_content[\"Title\"] % error_string, fontsize=\"xx-large\")\n",
    "        axes[i][j].tick_params(axis='both', which='major', labelsize=\"x-large\")\n",
    "        axes[i][j].tick_params(axis='both', which='minor', labelsize=\"x-large\")\n",
    "\n",
    "        for gap in table_content[\"Gaps\"]:\n",
    "            gap_lower = np.argmax(evaluation[\"Percentage Full [%%]\"] >= gap[0])\n",
    "            gap_upper = np.argmax(evaluation[\"Percentage Full [%%]\"] >= gap[1])\n",
    "            axes[i][j].axvspan(gap_lower, gap_upper, alpha=.1, color=\"#d7191c\")\n",
    "\n",
    "        table_content_index += 1\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window_size = 50\n",
    "\n",
    "n_rows = 2\n",
    "n_cols = int(len(table_contents) / n_rows)\n",
    "\n",
    "\n",
    "figure, axes = plt.subplots(nrows=n_rows, ncols=n_cols, sharex=False, sharey=True, figsize=(15,15),constrained_layout=True)\n",
    "\n",
    "\n",
    "table_content_index = 0\n",
    "\n",
    "gap_color = \"#e66101\"\n",
    "predictions_color = \"#b2abd2\"\n",
    "rolling_average_color = \"#5e3c99\"\n",
    "measurements_color = \"#fdb863\"\n",
    "\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_cols):\n",
    "            \n",
    "        table_content = table_contents[table_content_index]\n",
    "        \n",
    "        predictions = table_content[\"Predictions\"] * 100\n",
    "        rolling_predictions = table_content[\"Predictions_rolling\"] * 100\n",
    "        measurements = table_content[\"Measurements\"] * 100\n",
    "        \n",
    "        predictions.plot(ax=axes[i][j], color=predictions_color, alpha=.6)\n",
    "        rolling_predictions.plot(ax=axes[i][j], color=rolling_average_color)\n",
    "        measurements.plot(ax=axes[i][j], label=\"Measurements\", color=measurements_color)\n",
    "\n",
    "        if i == 0 and j == 0:\n",
    "            axes[i][j].legend([\"Predictions\", \"Predictions (Rolling Average)\", \"Measurements\"],loc='upper left', prop={'size': 18})\n",
    "        axes[i][j].set_ylabel(\"Percentage Full [%]\", fontsize=\"xx-large\")\n",
    "        axes[i][j].set_xlabel(\"Measurements\", fontsize=\"x-large\")\n",
    "        axes[i][j].set_title(table_content[\"Title\"] % \"\", fontsize=\"xx-large\")\n",
    "        \n",
    "        axes[i][j].tick_params(axis='both', which='major', labelsize=\"x-large\")\n",
    "        axes[i][j].tick_params(axis='both', which='minor', labelsize=\"x-large\")\n",
    "        \n",
    "        axes[i][j].set_xlim(0,len(measurements))\n",
    "        axes[i][j].set_ylim(40,100)\n",
    "        \n",
    "\n",
    "        for gap in table_content[\"Gaps\"]:\n",
    "            gap_lower = np.argmax(evaluation[\"Percentage Full [%%]\"] >= gap[0])\n",
    "            gap_upper = np.argmax(evaluation[\"Percentage Full [%%]\"] >= gap[1])\n",
    "            axes[i][j].axvspan(gap_lower, gap_upper, alpha=.1, color=gap_color)\n",
    "        \n",
    "        error_string = \"Error Summary: [%.2f, %.2f, %.2f]\" % (table_content[\"Percentiles\"][0.05], table_content[\"RRMSE\"], table_content[\"Percentiles\"][0.95])\n",
    "        error_string = \\\n",
    "            \"Prediction Errors:\\n\" + \\\n",
    "            \"5$^{th}$ percentile:\\t%.2f\\n\" % table_content[\"Percentiles\"][0.05] + \\\n",
    "            \"95$^{th}$ percentile: \\t%.2f\\n\" % table_content[\"Percentiles\"][0.95] + \\\n",
    "            \"RRMSE:\\t\\t%.2f\" % table_content[\"RRMSE\"]\n",
    "         \n",
    "        error_string =  f\"{'RRMSE:':<25}|{table_content['RRMSE']:>10.2f}\\n\"  +\\\n",
    "                        f\"{'5$^{th}$ percentile:':<25}|{table_content['Percentiles'][0.05]:>10.2f}\\n\" +\\\n",
    "                        f\"{'95$^{th}$ percentile:':<25}|{table_content['Percentiles'][0.95]:>10.2f}\"\n",
    "        \n",
    "        error_string =  f\"{table_content['RRMSE']:>10.2f} {'RRMSE':>25}\\n\"  +\\\n",
    "                        f\"{table_content['Percentiles'][0.05]:>10.2f}{'5$^{th}$ percentile':>32}\\n\" +\\\n",
    "                        f\"{table_content['Percentiles'][0.95]:>10.2f}{'95$^{th}$ percentile':>32}\"\n",
    "            \n",
    "        \n",
    "        axes[i][j].text(6000,45,error_string, fontsize = \"xx-large\", linespacing=1.1)\n",
    "\n",
    "        table_content_index += 1\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_blocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_contents = list()\n",
    "plot_scenario = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
