{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_errors(model_evaluation):\n",
    "    model_evaluation[absolute_prediction_error] = model_evaluation[velocity_column] - model_evaluation[predictions_colum]\n",
    "    model_evaluation[relative_prediction_error] = model_evaluation[absolute_prediction_error] / model_evaluation[velocity_column] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scenario += 1\n",
    "\n",
    "if plot_scenario == 1:\n",
    "    model_folder = \"DEFINE\"\n",
    "    gaps= []\n",
    "    scenario = \"Scenario 1\"\n",
    "    \n",
    "if plot_scenario == 2:\n",
    "    model_folder = \"DEFINE\"\n",
    "    gaps = [\n",
    "        (0.575, 0.625), # DEFINE, or REMOVE\n",
    "        (0.65, 0.75) # DEFINE, or REMOVE\n",
    "        ]\n",
    "    scenario = \"Scenario 2\"\n",
    "    \n",
    "if plot_scenario == 3:\n",
    "    model_folder = \"DEFINE\"\n",
    "    gaps = [\n",
    "        (0,.62), # DEFINE, or REMOVE\n",
    "        (0.73, 0.8048708335627524) # manually corrected from (0.73,1) # DEFINE, or REMOVE\n",
    "        ]\n",
    "    scenario = \"Scenario 3\"\n",
    "\n",
    "if plot_scenario == 4:\n",
    "    model_folder = \"DEFINE\"\n",
    "    gaps = [\n",
    "        (.62,.73) # DEFINE, or REMOVE\n",
    "        ]\n",
    "    scenario = \"Scenario 4\"\n",
    "    \n",
    "    \n",
    "positions = [\"pos3\"]\n",
    "\n",
    "absolute_prediction_error = \"Absolute Prediction Error [m/s]\"\n",
    "relative_prediction_error = \"Relative Prediction Error [%]\"\n",
    "velocity_column = \"Velocity\"\n",
    "predictions_colum = \"Predictions\"\n",
    "\n",
    "all_models = list()\n",
    "model_performance = list()\n",
    "best_model_mean = None\n",
    "best_model_range = None\n",
    "best_absolute_mean = None\n",
    "\n",
    "best_absolute_quantile_range = None\n",
    "\n",
    "evaluation_key = \"_evaluation_set_result.csv\"\n",
    "\n",
    "for current_experiment in positions:\n",
    "    \n",
    "    for file in os.listdir(model_folder):\n",
    "\n",
    "        if not current_experiment in file:\n",
    "            continue\n",
    "\n",
    "        if evaluation_key in file:\n",
    "\n",
    "            model_name_end_index = file.index(evaluation_key)\n",
    "\n",
    "            model_name = file[0:model_name_end_index]\n",
    "            all_models.append(model_name)\n",
    "\n",
    "    for model in all_models:\n",
    "        \n",
    "        if not \"m_4_conv_5\" in model:\n",
    "            continue\n",
    "\n",
    "        evaluation_path = model_folder + model + evaluation_key\n",
    "        evaluation = pd.read_csv(evaluation_path)\n",
    "\n",
    "        _compute_errors(evaluation)\n",
    "\n",
    "        mean = evaluation[relative_prediction_error].pow(2).apply(np.sqrt).mean()\n",
    "        model_performance.append((mean, model))\n",
    "        \n",
    "        absolute_mean = abs(mean)\n",
    "\n",
    "        quantiles = evaluation[relative_prediction_error].quantile([.05, .95])\n",
    "\n",
    "        quantile_range = quantiles[0.95] - quantiles[0.05]\n",
    "\n",
    "        print(\"%s mean error: %.2f\" % (model, mean))\n",
    "        print(\"%s quantile range: %.2f\\n\" % (model, quantile_range))\n",
    "\n",
    "        if best_absolute_mean is None or absolute_mean < best_absolute_mean:\n",
    "            best_absolute_mean = absolute_mean\n",
    "            best_model_mean = model\n",
    "\n",
    "        if best_absolute_quantile_range is None or quantile_range < best_absolute_quantile_range:\n",
    "            best_absolute_quantile_range = quantile_range\n",
    "            best_model_range = model\n",
    "\n",
    "    print(\"Best model (mean) is: %s with mean error: %.2f\" % (best_model_mean, best_absolute_mean))\n",
    "    print(\"Best model (quantile range) is: %s with quantile range: %.2f\" % (best_model_range, best_absolute_quantile_range))\n",
    "\n",
    "model_name = best_model_range\n",
    "model_name = best_model_mean\n",
    "\n",
    "evaluation_path = model_folder + model_name + evaluation_key\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = np.array(model_performance)\n",
    "performance = pd.DataFrame({\"Name\" : performance[:,1], \"Error\": performance[:,0]})\n",
    "performance = performance.astype({\"Error\" : \"float\"})\n",
    "tmp_sorted = performance.sort_values(by=\"Error\")\n",
    "tmp_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_complete = pd.read_csv(evaluation_path)\n",
    "_compute_errors(evaluation_complete)\n",
    "\n",
    "evaluation_complete = evaluation_complete.sort_values(by=\"Frame\", ignore_index=True)\n",
    "\n",
    "evaluation_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.read_csv(evaluation_path)\n",
    "_compute_errors(evaluation)\n",
    "\n",
    "evaluation = evaluation[[velocity_column, predictions_colum, absolute_prediction_error, relative_prediction_error]]\n",
    "\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_quantile = .05\n",
    "upper_quantile = .95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_error_quantiles = evaluation[absolute_prediction_error].quantile([lower_quantile, upper_quantile])\n",
    "absolute_error_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_error_quantiles = evaluation[relative_prediction_error].quantile([lower_quantile, upper_quantile])\n",
    "relative_error_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrmse = evaluation[relative_prediction_error].pow(2).apply(np.sqrt).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window_size = 50\n",
    "\n",
    "figure, axes = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(15,10))\n",
    "figure.tight_layout()\n",
    "\n",
    "evaluation[velocity_column].plot(ax=axes[0], label=\"Measurements\", title=\"Measurements vs. Predictions\")\n",
    "evaluation[predictions_colum].plot(ax=axes[0])\n",
    "\n",
    "axes[0].legend()\n",
    "\n",
    "evaluation[absolute_prediction_error].plot(ax=axes[1], title=absolute_prediction_error)\n",
    "evaluation[absolute_prediction_error].rolling(rolling_window_size).mean().plot(ax=axes[1], title=absolute_prediction_error)\n",
    "\n",
    "axes[1].axhline(evaluation[absolute_prediction_error].mean(), color=\"green\")\n",
    "axes[1].axhline(absolute_error_quantiles[lower_quantile], color=\"red\")\n",
    "axes[1].axhline(absolute_error_quantiles[upper_quantile], color=\"red\")\n",
    "\n",
    "axes[1].axhline(min(evaluation[absolute_prediction_error]), color=\"black\")\n",
    "axes[1].axhline(max(evaluation[absolute_prediction_error]), color=\"black\")\n",
    "\n",
    "evaluation[relative_prediction_error].plot(ax=axes[2], title=relative_prediction_error)\n",
    "evaluation[relative_prediction_error].rolling(rolling_window_size).mean().plot(ax=axes[2], title=relative_prediction_error)\n",
    "\n",
    "axes[2].axhline(evaluation[relative_prediction_error].mean(), color=\"green\")\n",
    "axes[2].axhline(relative_error_quantiles[lower_quantile], color=\"red\")\n",
    "axes[2].axhline(relative_error_quantiles[upper_quantile], color=\"red\")\n",
    "\n",
    "axes[2].axhline(min(evaluation[relative_prediction_error]), color=\"black\")\n",
    "axes[2].axhline(max(evaluation[relative_prediction_error]), color=\"black\")\n",
    "\n",
    "for axis in axes:\n",
    "    for gap in gaps:\n",
    "        gap_lower = np.argmax(evaluation[velocity_column] >= gap[0])\n",
    "        gap_upper = np.argmax(evaluation[velocity_column] >= gap[1])\n",
    "        axis.axvspan(gap_lower, gap_upper, alpha=.1, color=\"#d7191c\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window_size = 50\n",
    "\n",
    "figure, axes = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(15,10))\n",
    "figure.tight_layout()\n",
    "\n",
    "evaluation[velocity_column].plot(ax=axes[0], label=\"Measurements\", title=\"Measurements vs. Predictions\", fontsize=\"x-large\")\n",
    "evaluation[predictions_colum].plot(ax=axes[0])\n",
    "\n",
    "axes[0].legend()\n",
    "axes[0].set_ylabel(\"Surface Velocity [m/s]\", fontsize=\"x-large\")\n",
    "\n",
    "evaluation[absolute_prediction_error].plot(ax=axes[1], title=absolute_prediction_error)\n",
    "evaluation[absolute_prediction_error].rolling(rolling_window_size).mean().plot(ax=axes[1], title=absolute_prediction_error)\n",
    "\n",
    "evaluation[relative_prediction_error].plot(ax=axes[1], title=relative_prediction_error)\n",
    "evaluation[relative_prediction_error].rolling(rolling_window_size).mean().plot(ax=axes[1], title=relative_prediction_error, fontsize=\"x-large\")\n",
    "\n",
    "axes[1].axhline(evaluation[relative_prediction_error].mean(), color=\"green\")\n",
    "axes[1].axhline(relative_error_quantiles[lower_quantile], color=\"red\")\n",
    "axes[1].axhline(relative_error_quantiles[upper_quantile], color=\"red\")\n",
    "\n",
    "axes[1].axhline(min(evaluation[relative_prediction_error]), color=\"black\")\n",
    "axes[1].axhline(max(evaluation[relative_prediction_error]), color=\"black\")\n",
    "\n",
    "axes[1].set_xlabel(\"Measurement\", fontsize=\"x-large\")\n",
    "axes[1].set_ylabel(\"Error [%]\", fontsize=\"x-large\")\n",
    "\n",
    "plt.subplots_adjust(hspace = .2)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window_size = 10\n",
    "\n",
    "figure, axes = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(15,10))\n",
    "figure.tight_layout()\n",
    "\n",
    "evaluation[predictions_colum].plot(ax=axes, color=\"#abd9e9\", alpha=.6)\n",
    "evaluation[predictions_colum].rolling(rolling_window_size).mean().plot(ax=axes, color=\"#2c7bb6\")\n",
    "evaluation[velocity_column].plot(ax=axes, label=\"Measurements\", color=\"#d7191c\", fontsize=\"x-large\")\n",
    "\n",
    "\n",
    "axes.legend([\"Predictions\", \"Predictions (Rolling Average)\", \"Measurements\"])\n",
    "axes.set_ylabel(\"Surface Velocity [m/s]\", fontsize=\"x-large\")\n",
    "axes.set_xlabel(\"# Measurements\", fontsize=\"x-large\")\n",
    "    \n",
    "for axis in [axes]:\n",
    "    for gap in gaps:\n",
    "        gap_lower = np.argmax(evaluation[velocity_column] >= gap[0])\n",
    "        gap_upper = np.argmax(evaluation[velocity_column] >= gap[1])\n",
    "        axis.axvspan(gap_lower, gap_upper, alpha=.1, color=\"#d7191c\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean relative prediction error: %.2f\" % evaluation[relative_prediction_error].mean())\n",
    "print(\"Mean relative prediction error 5th percentile: %.2f%%\" % relative_error_quantiles[lower_quantile])\n",
    "print(\"Mean relative prediction error 95th percentile: %.2f%%\" % relative_error_quantiles[upper_quantile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_bins = 21 # 5% steps\n",
    "bin_percentages = np.linspace(0,100,21)\n",
    "bin_labels = [\"L%d\" % i for i in bin_percentages]\n",
    "percentage_label = \"Percentage Label\"\n",
    "\n",
    "labels, bins = pd.cut(evaluation[velocity_column], number_of_bins, labels=bin_labels, retbins=True)\n",
    "evaluation[percentage_label] = labels\n",
    "\n",
    "group_means = evaluation.groupby(by=percentage_label).mean()\n",
    "group_quantiles = evaluation[[percentage_label, relative_prediction_error]].groupby(by=percentage_label).quantile([.05, .95])\n",
    "\n",
    "group_quantiles.index.set_names([\"Label\", \"Quantile\"], inplace=True)\n",
    "\n",
    "upper_quantiles = group_quantiles.iloc[group_quantiles.index.get_level_values('Quantile') == upper_quantile]\n",
    "lower_quantiles = group_quantiles.iloc[group_quantiles.index.get_level_values('Quantile') == lower_quantile]\n",
    "\n",
    "lower = lower_quantiles[relative_prediction_error].values\n",
    "upper = upper_quantiles[relative_prediction_error].values\n",
    "means = group_means[relative_prediction_error].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10/1.6))\n",
    "fig.tight_layout()\n",
    "\n",
    "zero_line_y = np.zeros(len(bin_labels))\n",
    "\n",
    "plt.plot(bin_labels, lower)\n",
    "plt.plot(bin_labels, upper)\n",
    "plt.plot(bin_labels, means)\n",
    "\n",
    "plt.plot(bin_labels, zero_line_y, alpha=0)\n",
    "plt.fill_between(bin_labels, lower, upper, facecolor=\"C0\", alpha=0.05)\n",
    "plt.fill_between(bin_labels, zero_line_y, means, where=(zero_line_y < means), facecolor=\"C1\", alpha=0.35)\n",
    "plt.fill_between(bin_labels, zero_line_y, means, where=(zero_line_y > means), facecolor=\"C2\", alpha=0.35)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 10\n",
    "\n",
    "new_bin_indexes = np.linspace(0, 100, number_of_bins * factor)\n",
    "\n",
    "lower_inderpolated = np.interp(new_bin_indexes, bin_percentages, lower)\n",
    "upper_inderpolated = np.interp(new_bin_indexes, bin_percentages, upper)\n",
    "means_inderpolated = np.interp(new_bin_indexes, bin_percentages, means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10/1.6))\n",
    "fig.tight_layout()\n",
    "\n",
    "zero_line_y = np.zeros(len(lower_inderpolated))\n",
    "\n",
    "plt.plot(new_bin_indexes, lower_inderpolated)\n",
    "plt.plot(new_bin_indexes, upper_inderpolated)\n",
    "plt.plot(new_bin_indexes, means_inderpolated)\n",
    "\n",
    "plt.ylabel(\"Relative Error (Expected-Prediction) [%%]\")\n",
    "plt.xlabel(\"Percentage Full [%]\")\n",
    "\n",
    "plt.plot(new_bin_indexes, zero_line_y, alpha=0)\n",
    "plt.fill_between(new_bin_indexes, lower_inderpolated, upper_inderpolated, facecolor=\"C0\", alpha=0.05)\n",
    "plt.fill_between(new_bin_indexes, zero_line_y, means_inderpolated, where=(zero_line_y < means_inderpolated), facecolor=\"C1\", alpha=0.35)\n",
    "plt.fill_between(new_bin_indexes, zero_line_y, means_inderpolated, where=(zero_line_y > means_inderpolated), facecolor=\"C2\", alpha=0.35)\n",
    "\n",
    "plt.xticks((bin_percentages))\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_entry_unsorted = {\n",
    "    \"Title\" : scenario,\n",
    "    \"Name\" : model_name,\n",
    "    \"RRMSE\" : rrmse,\n",
    "    \"Percentiles\" : relative_error_quantiles,\n",
    "    \"Measurements\" : evaluation[velocity_column],\n",
    "    \"Predictions\" : evaluation[predictions_colum],\n",
    "    \"Gaps\" : gaps\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_contents_unsorted.append(table_entry_unsorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = evaluation.sort_values(by=velocity_column, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_entry = {\n",
    "    \"Title\" : scenario,\n",
    "    \"Name\" : model_name,\n",
    "    \"RRMSE\" : rrmse,\n",
    "    \"Percentiles\" : relative_error_quantiles,\n",
    "    \"Measurements\" : evaluation[velocity_column],\n",
    "    \"Predictions\" : evaluation[predictions_colum],\n",
    "    \"Gaps\" : gaps\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_contents.append(table_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in table_contents:\n",
    "    print(t[\"Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(table_contents) > 3):\n",
    "    backup = table_contents[2][\"Gaps\"]\n",
    "    backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(table_contents) > 3):\n",
    "    table_contents[2][\"Gaps\"] = [(0, 0.62), (0.73, max(evaluation[velocity_column]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_contents[0][\"Title\"] = \"Scenario 1 %s\"\n",
    "table_contents[1][\"Title\"] = \"Scenario 2 %s\"\n",
    "table_contents[2][\"Title\"] = \"Scenario 3 %s\"\n",
    "table_contents[3][\"Title\"] = \"Scenario 4 %s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window_size = 10\n",
    "\n",
    "n_rows = 2\n",
    "n_cols = int(len(table_contents) / n_rows)\n",
    "\n",
    "figure, axes = plt.subplots(nrows=n_rows, ncols=n_cols, sharex=False, sharey=True, figsize=(15,15),constrained_layout=True)\n",
    "\n",
    "table_content_index = 0\n",
    "\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_cols):\n",
    "        \n",
    "        table_content = table_contents[table_content_index]\n",
    "        error_string = \"\\n[%.2f, %.2f, %.2f]\" % (table_content[\"Percentiles\"][0.05], table_content[\"RRMSE\"], table_content[\"Percentiles\"][0.95])\n",
    "\n",
    "        table_content[\"Predictions\"].plot(ax=axes[i][j], color=\"#abd9e9\", alpha=.6)\n",
    "        table_content[\"Predictions\"].rolling(rolling_window_size).mean().plot(ax=axes[i][j], color=\"#2c7bb6\")\n",
    "        table_content[\"Measurements\"].plot(ax=axes[i][j], label=\"Measurements\", color=\"#fdae61\")\n",
    "\n",
    "        axes[i][j].legend([\"Predictions\", \"Predictions (Rolling Average)\", \"Measurements\"],loc='upper left', prop={'size': 18})\n",
    "        axes[i][j].set_ylabel(\"Surface Velocity [m/s]\", fontsize=\"xx-large\")\n",
    "        axes[i][j].set_xlabel(\"Measurements\", fontsize=\"x-large\")\n",
    "#         axes[i][j].set_xlabel(\"Measurements\\n%s\" % error_string, fontsize=\"x-large\")\n",
    "        axes[i][j].set_title(table_content[\"Title\"] % error_string, fontsize=\"x-large\")\n",
    "    \n",
    "        axes[i][j].tick_params(axis='both', which='major', labelsize=\"x-large\")\n",
    "        axes[i][j].tick_params(axis='both', which='minor', labelsize=\"x-large\")\n",
    "    \n",
    "        #axes[ax_index].grid()\n",
    "        \n",
    "\n",
    "        for gap in table_content[\"Gaps\"]:\n",
    "            gap_lower = np.argmax(evaluation[velocity_column] >= gap[0])\n",
    "            gap_upper = np.argmax(evaluation[velocity_column] >= gap[1])\n",
    "        \n",
    "            axes[i][j].axvspan(gap_lower, gap_upper, alpha=.1, color=\"#d7191c\")\n",
    "\n",
    "        table_content_index += 1\n",
    "\n",
    "figure.suptitle(u\"Surface Velocity Prediction Scenario Evaluation\", fontsize=\"20\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window_size = 10\n",
    "\n",
    "n_rows = 2\n",
    "n_cols = int(len(table_contents) / n_rows)\n",
    "\n",
    "\n",
    "figure, axes = plt.subplots(nrows=n_rows, ncols=n_cols, sharex=False, sharey=True, figsize=(15,15),constrained_layout=True)\n",
    "\n",
    "table_content_index = 0\n",
    "\n",
    "gap_color = \"#e66101\"\n",
    "predictions_color = \"#b2abd2\"\n",
    "rolling_average_color = \"#5e3c99\"\n",
    "measurements_color = \"#fdb863\"\n",
    "\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_cols):\n",
    "            \n",
    "        table_content = table_contents[table_content_index]\n",
    "        \n",
    "        table_content[\"Predictions\"].plot(ax=axes[i][j], color=predictions_color, alpha=.6)\n",
    "        table_content[\"Predictions\"].rolling(rolling_window_size).mean().plot(ax=axes[i][j], color=rolling_average_color)\n",
    "        table_content[\"Measurements\"].plot(ax=axes[i][j], label=\"Measurements\", color=measurements_color)\n",
    "\n",
    "        if i == 0 and j == 0:\n",
    "            axes[i][j].legend([\"Predictions\", \"Predictions (Rolling Average)\", \"Measurements\"],loc='upper left', prop={'size': 18})\n",
    "        axes[i][j].set_ylabel(\"Surface Velocity [m/s]\", fontsize=\"xx-large\")\n",
    "        axes[i][j].set_xlabel(\"Measurements\", fontsize=\"x-large\")\n",
    "        axes[i][j].set_title(table_content[\"Title\"] % \"\", fontsize=\"xx-large\")\n",
    "        \n",
    "        axes[i][j].tick_params(axis='both', which='major', labelsize=\"x-large\")\n",
    "        axes[i][j].tick_params(axis='both', which='minor', labelsize=\"x-large\")\n",
    "        \n",
    "        axes[i][j].set_xlim(0,len(table_content[\"Measurements\"]))\n",
    "        axes[i][j].set_ylim(table_content[\"Measurements\"].min(),table_content[\"Measurements\"].max())\n",
    "        \n",
    "\n",
    "        for gap in table_content[\"Gaps\"]:\n",
    "            gap_lower = np.argmax(evaluation[velocity_column] >= gap[0])\n",
    "            gap_upper = np.argmax(evaluation[velocity_column] >= gap[1])\n",
    "            axes[i][j].axvspan(gap_lower, gap_upper, alpha=.1, color=gap_color)\n",
    "        \n",
    "        error_string = \"Error Summary: [%.2f, %.2f, %.2f]\" % (table_content[\"Percentiles\"][0.05], table_content[\"RRMSE\"], table_content[\"Percentiles\"][0.95])\n",
    "        error_string = \\\n",
    "            \"Prediction Errors:\\n\" + \\\n",
    "            \"5$^{th}$ percentile:\\t%.2f\\n\" % table_content[\"Percentiles\"][0.05] + \\\n",
    "            \"95$^{th}$ percentile: \\t%.2f\\n\" % table_content[\"Percentiles\"][0.95] + \\\n",
    "            \"RRMSE:\\t\\t%.2f\" % table_content[\"RRMSE\"]\n",
    "        \n",
    "        error_string =  f\"{'RRMSE:':<25}|{table_content['RRMSE']:>10.2f}\\n\"  +\\\n",
    "                        f\"{'5$^{th}$ percentile:':<25}|{table_content['Percentiles'][0.05]:>10.2f}\\n\" +\\\n",
    "                        f\"{'95$^{th}$ percentile:':<25}|{table_content['Percentiles'][0.95]:>10.2f}\"\n",
    "        \n",
    "        error_string =  f\"{table_content['RRMSE']:>10.2f} {'RRMSE':>25}\\n\"  +\\\n",
    "                        f\"{table_content['Percentiles'][0.05]:>10.2f}{'5$^{th}$ percentile':>30}\\n\" +\\\n",
    "                        f\"{table_content['Percentiles'][0.95]:>10.2f}{'95$^{th}$ percentile':>29}\"\n",
    "            \n",
    "        \n",
    "        axes[i][j].text(200,.5,error_string, fontsize = \"xx-large\", linespacing=1.1)\n",
    "\n",
    "        table_content_index += 1\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_content = table_contents_unsorted[0]\n",
    "\n",
    "table_content[\"Predictions\"] = pd.DataFrame(table_content[\"Predictions\"])\n",
    "table_content[\"Measurements\"] = pd.DataFrame(table_content[\"Measurements\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table_content[\"Title\"] )\n",
    "print(float(table_content[\"Measurements\"].min()))\n",
    "print(float(table_content[\"Measurements\"].max()))\n",
    "\n",
    "n_rows = 1\n",
    "n_cols = 1\n",
    "\n",
    "rolling_window_size = 10\n",
    "\n",
    "figure, axes = plt.subplots(nrows=n_rows, ncols=n_cols, sharex=False, sharey=True, figsize=(10,10),constrained_layout=True)\n",
    "\n",
    "gap_color = \"#e66101\"\n",
    "predictions_color = \"#b2abd2\"\n",
    "rolling_average_color = \"#5e3c99\"\n",
    "measurements_color = \"#fdb863\"\n",
    "\n",
    "predictions = table_content[\"Predictions\"] \n",
    "rolling_predictions = table_content[\"Predictions\"].rolling(rolling_window_size).mean() \n",
    "measurements = table_content[\"Measurements\"].rolling(rolling_window_size).mean() \n",
    "        \n",
    "rolling_predictions.plot(ax=axes, color=rolling_average_color)\n",
    "measurements.plot(ax=axes, label=\"Measurements\", color=measurements_color)\n",
    "\n",
    "axes.legend([\"Predictions\", \"Predictions (Rolling Average)\", \"Measurements\"],loc='upper right', prop={'size': 18})\n",
    "axes.set_ylabel(\"Surface Velocity [m/s]\", fontsize=\"xx-large\")\n",
    "axes.set_xlabel(\"Time[s]\", fontsize=\"x-large\")\n",
    "axes.set_title(table_content[\"Title\"], fontsize=\"xx-large\")\n",
    "        \n",
    "axes.tick_params(axis='both', which='major', labelsize=\"x-large\")\n",
    "axes.tick_params(axis='both', which='minor', labelsize=\"x-large\")\n",
    "        \n",
    "max_x = table_content[\"Measurements\"].index.max()\n",
    "axes.set_xlim(0, table_content[\"Measurements\"].index.max())\n",
    "axes.set_ylim(float(table_content[\"Measurements\"].min()), float(table_content[\"Measurements\"].max()))\n",
    "\n",
    "error_string =  f\"{table_content['RRMSE']:>10.2f} {'RRMSE':>25}\\n\"  +\\\n",
    "                f\"{table_content['Percentiles'][0.05]:>10.2f}{'5$^{th}$ percentile':>30}\\n\" +\\\n",
    "                f\"{table_content['Percentiles'][0.95]:>10.2f}{'95$^{th}$ percentile':>29}\"\n",
    "            \n",
    "        \n",
    "axes.text(200,.5,error_string, fontsize = \"xx-large\", linespacing=1.1)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(table_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_contents[0][\"Title\"] = \"Scenario 1 %s\"\n",
    "table_contents[1][\"Title\"] = \"Scenario 2 %s\"\n",
    "table_contents[2][\"Title\"] = \"Scenario 3 %s\"\n",
    "table_contents[3][\"Title\"] = \"Scenario 4 %s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_blocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_contents = list()\n",
    "table_contents_unsorted = list()\n",
    "plot_scenario = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
